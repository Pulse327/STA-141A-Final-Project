---
title: "STA 141A Final Project Report"
author: "Yuanchun Liu"
date: "2024-03-13"
output: html_document
---
```{r setup, echo=FALSE, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r, echo=FALSE}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(readr)
library(tidyverse)
library(caret) 
library(xgboost)
library(pROC)
library(knitr)

```

# Abstract

In this report, I present a predictive model developed from neural activity data and visual stimuli contrasts. I want to determine the feedback outcomes of behavioral trials in mice,which utilizes a subset of the extensive dataset by Steinmetz et al. (2019). My analysis has three phases: an exploratory examination of neural patterns and session characteristics, integration of data across sessions to identify shared patterns and discrepancies, and the construction and validation of a predictive model. The model's performance would be evaluated on two distinct test sets, which provide a measure of its accuracy in anticipating trial outcomes.

# Introduction

The 2019 study by Steinmetz and colleagues was a great exploration into how mice process visual information and make decisions, with the neural activity during these tasks captured in extensive detail. This provided an intricate data set where the firing patterns of neurons were linked to the mice's ability to discern varying contrasts on screens, resulting in either rewards or penalties based on their choices. Building on this seminal work, this study focuses on crafting a predictive model that aims to predict the outcomes of such behavioral trials by delving into the neural spike data. This model connects between the flurry of neural activity and the feedback received during trials, which study into how decision-making is encoded in the brain. 

Five variables are available for each trial, namely:

feedback_type: type of the feedback, 1 for success and -1 for failure

contrast_left: contrast of the left stimulus

contrast_right: contrast of the right stimulus

time: centers of the time bins for spks

spks: numbers of spikes of neurons in the visual cortex in time bins defined in time

brain_area: area of the brain where each neuron lives.

To get more detailed information, I created the following table in order to show the mouse name, date exp n_brain_area, n_neurons n_trials, success_rate for each of the 18 sessions.

```{r,echo=FALSE}
session <- list()
for(i in 1:18){
  session[[i]] <- readRDS(paste('./Data/session', i, '.rds', sep=''))
  cat('Session', i, ':', session[[i]]$mouse_name, 'on', session[[i]]$date_exp, '\n')
}

library(tidyverse)
n.session=length(session)

meta <- tibble(
  mouse_name = rep('name',n.session),
  date_exp =rep('dt',n.session),
  n_brain_area = rep(0,n.session),
  n_neurons = rep(0,n.session),
  n_trials = rep(0,n.session),
  success_rate = rep(0,n.session),
)


for(i in 1:n.session){
  
  tmp = session[[i]];
  meta[i,1]=tmp$mouse_name;
  meta[i,2]=tmp$date_exp;
  meta[i,3]=length(unique(tmp$brain_area));
  meta[i,4]=dim(tmp$spks[[1]])[1];
  meta[i,5]=length(tmp$feedback_type);
  meta[i,6]=mean(tmp$feedback_type+1)/2;
  }
kable(meta, format = "html", table.attr = "class='table table-striped'",digits=2)
```
The dataset includes 18 sessions, which captures neural data from four mice: Cori (Sessions 1-3, Dec 2016), Forssmann (Sessions 4-7, Nov 2017), Hench (Sessions 8-11, Jun 2017), and Lederberg (Sessions 12-18, Dec 2017).

# Data Processiong and Exploratory Analysis

## (1) Data Processing

I start by identifying the dimensions of neural spikes data, which reflects the number of neurons and the timeframe for each trial. I also count the distinct brain areas monitored. This gives insights into the diversity of the neural data.

The get_trail_data function sums and averages the spikes for each neuron. This data is then augmented with additional trial information such as stimulus contrasts and feedback types. For a broader view, get_session_data collates this information across all trials in a session. I repeat this process across all sessions, culminating in a comprehensive dataset.

```{r,echo=FALSE}
dim(session[[1]]$spks[[1]]) 
length(session[[1]]$brain_area)
session[[1]]$spks[[1]][6,] 

# Spike and brain region connection
session[[1]]$spks[[1]][6,3] 
session[[1]]$brain_area[6]

# Data processing functions
binename <- paste0("bin", as.character(1:40))
get_trail_data <- function(session_id, trail_id){
  spikes = session[[session_id]]$spks[[trail_id]]
  if (any(is.na(spikes))){
    disp("value missing")
  }

  trail_tibble <- tibble("neuron_spike" = rowSums(spikes)) %>%  
    add_column("brain_area" = session[[session_id]]$brain_area ) %>%
    group_by(brain_area) %>%
    summarize( 
      region_sum_spike = sum(neuron_spike), 
      region_count = n(),
      region_mean_spike = mean(neuron_spike)
    ) %>%
    add_column("trail_id" = trail_id) %>%
    add_column("contrast_left"= session[[session_id]]$contrast_left[trail_id]) %>%
    add_column("contrast_right"= session[[session_id]]$contrast_right[trail_id]) %>%
    add_column("feedback_type"= session[[session_id]]$feedback_type[trail_id])
  
  trail_tibble
}

get_session_data <- function(session_id){
  n_trail = length(session[[session_id]]$spks)
  trail_list = list()
  for (trail_id in 1:n_trail){
    trail_tibble = get_trail_data(session_id,trail_id)
    trail_list[[trail_id]] = trail_tibble
  }
  session_tibble = do.call(rbind, trail_list)
  session_tibble = session_tibble %>%
    add_column("mouse_name" = session[[session_id]]$mouse_name) %>%
    add_column("date_exp" = session[[session_id]]$date_exp) %>%
    add_column("session_id" = session_id)
  
  session_tibble
}

# Example usage of processing functions
trail_tibble_1_2 = get_trail_data(1,2)
print(trail_tibble_1_2)

session_1 = get_session_data(1)
print(head(session_1))

# data from all sessions
session_list = list()
for (session_id in 1:18){
  session_list[[session_id]] = get_session_data(session_id)
}
full_tibble = do.call(rbind, session_list)

```


The data shows neural activity for Cori on December 14, 2016, across six brain areas during one trial. The average spikes per neuron ranged from 0.94 in MOs to 2.32 in DG. This trial had a right contrast of 0.5, left contrast of 0, and resulted in a successful feedback.

## (2) Difference between sessions/mouses    

I explore the number of neuron, the unique brain areas, and the average spike rate in each session. 
```{r,echo=FALSE}
# number of neuron in each region
full_tibble %>% filter(trail_id==1) %>% group_by(session_id) %>% summarize(sum(region_count))
# Brain areas per session
full_tibble %>% group_by(session_id) %>% summarize(unique_area = n_distinct(brain_area))
# Average spike rate over each session
average_spike = full_tibble %>% group_by(session_id, trail_id) %>% mutate(mean_spike = sum(region_sum_spike)/sum(region_count))
average_spike %>% group_by(session_id) %>% summarize(mean_session_spike = mean(mean_spike))

```
### Output Interpretation:


#### a. Neuron Sum

**Cori**: it shows an increase in neurons from Session 1 to Session 2, reflecting an adaptive response to the task. Then followed by a decrease in Session 3, which possibly indicates the task becomes more familiar.

**Forssmann**: it starts with a very high neuron count, which suggests intense neural activity and complex processing of the task. The sharp decline in the following sessions could reflect a learning process, during which the mouse becomes more efficient in task handling.

**Hench**: the data shows variability. The peak in Session 10 could suggests periods of increased neural engagement or complexity in performing the task. The fluctuation reflects the mouse's varying levels of attention.

**Lederberg**: it shows significant variability with no clear upward or downward trend until a sharp increase in the last session. This could indicate fluctuating engagement levels or adaptations to the task. 


#### b. Brain areas 


**Cori**: its variability may suggests adaptation or optimization in strategy over sessions.

**Forssmann**:it demonstrates an initial broad engagement that becomes more focused, which possibility indicates learning optimization.

**Hench**: it starts with highly distributed neural processing and later becomes more focused, suggesting a major adaptation in strategy.

**Lederberg**: it shows the most dynamic pattern with fluctuating levels of distributed processing, indicating continuous adaptation. 


#### c. Average spikes over each session


**Cori**: it demonstrates an increasing trend in neuronal activity, which possibly indicates heightened engagement.

**Forssmann**: it shows variable engagement levels across sessions with changes in neuronal activity that might reflect adaptation.

**Hench**: it starts with higher neuronal activity that decreases over time, suggesting increased efficiency or familiarity with the task.

**Lederberg**: it exhibits fluctuating engagement, with a peak in activity suggesting intense processing or effort, and later followed by adaptation. 


## (3) Neural Spike rate difference across trials 

By analyzing neural spike rates and stimuli contrasts across all sessions, I want to identify consistent patterns that emerge in the neural responses.

```{r,echo=FALSE}
ggplot(average_spike, aes(x = trail_id, y = mean_spike)) + 
  geom_line()+
  geom_smooth(method = "loess")+  # Fit a smooth spline

  facet_wrap(~session_id)
```

### Output Interpretation

**Sessions 1, 2, 3 (Cori)**: High variability in spike rates is seen across trials, and Session 3 exhibits the highest amplitude of fluctuations. This suggests that Cori's neuronal response varied greatly from trial to trial, indicating significant changes in engagement.

**Sessions 4, 5, 6, 7 (Forssmann)**: Session 4 starts with lower variability which increases in Session 5. Session 6 has notably low and stable spike rates, while Session 7 shows increased variability. Forssmann may be experiencing a stabilization in neural response by Session 6, which could suggest a phase of learning consolidation.

**Sessions 8, 9, 10, 11 (Hench)**: These sessions are characterized by relatively high variability. Session 8 has the highest mean spike rate and fluctuation, which indicates intense neuronal activity. The following sessions show a slight decline in this variability, which could imply a gradual adaptation to the task.

**Sessions 12 through 18 (Lederberg)**: Session 13 showing particularly large fluctuations in spike rate. The variability suggests that Lederberg's neuronal response to the task varies significantly from trial to trial, which may reflect an ongoing process of learning in task engagement.

# Data Integration

I performed additional data processing to prepare for integration: take the average of neuron spikes over each time bin, and use session_id, trail_id, signals, and the average spike rate of each time bin to see performance. To improve performance, I choose to remove session_id and trail_id from prediction. 

```{r,echo=FALSE}
binename <- paste0("bin", as.character(1:40))

get_trail_functional_data <- function(session_id, trail_id){
  spikes <- session[[session_id]]$spks[[trail_id]]
  if (any(is.na(spikes))){
    disp("value missing")
  }

  trail_bin_average <- matrix(colMeans(spikes), nrow = 1)
  colnames(trail_bin_average) <- binename
  trail_tibble  = as_tibble(trail_bin_average)%>% add_column("trail_id" = trail_id) %>% add_column("contrast_left"= session[[session_id]]$contrast_left[trail_id]) %>% add_column("contrast_right"= session[[session_id]]$contrast_right[trail_id]) %>% add_column("feedback_type"= session[[session_id]]$feedback_type[trail_id])
  
  trail_tibble
}
get_session_functional_data <- function(session_id){
  n_trail <- length(session[[session_id]]$spks)
  trail_list <- list()
  for (trail_id in 1:n_trail){
    trail_tibble <- get_trail_functional_data(session_id,trail_id)
    trail_list[[trail_id]] <- trail_tibble
  }
  session_tibble <- as_tibble(do.call(rbind, trail_list))
  session_tibble <- session_tibble %>% add_column("mouse_name" = session[[session_id]]$mouse_name) %>% add_column("date_exp" = session[[session_id]]$date_exp) %>% add_column("session_id" = session_id) 
  session_tibble
}
```

```{r, echo = FALSE}
session_list = list()
for (session_id in 1: 18){
  session_list[[session_id]] <- get_session_functional_data(session_id)
}
full_functional_tibble <- as_tibble(do.call(rbind, session_list))
full_functional_tibble$session_id <- as.factor(full_functional_tibble$session_id )
full_functional_tibble$contrast_diff <- abs(full_functional_tibble$contrast_left-full_functional_tibble$contrast_right)

full_functional_tibble$success <- full_functional_tibble$feedback_type == 1
full_functional_tibble$success <- as.numeric(full_functional_tibble$success)

predictive_feature <- c("contrast_right","contrast_left", "contrast_diff" ,binename)
head(full_functional_tibble[predictive_feature])

predictive_dat <- full_functional_tibble[predictive_feature]
  
label <- as.numeric(full_functional_tibble$success)
X <- model.matrix(~., predictive_dat)

```
# Prediction

Using XGBoost, I train a model on 80% of the data to predict feedback outcomes. The model's accuracy and AUROC score evaluate its performance. To ensure the model's robustness, I also test it on specific sessions, checking its generalizability.

```{r, echo=FALSE}
# split
library(caret)
set.seed(123) # for reproducibility
trainIndex <- createDataPartition(label, p = .8, 
                                  list = FALSE, 
                                  times = 1)
train_df <- predictive_dat[trainIndex, ]
train_X <- X[trainIndex,]
test_df <- predictive_dat[-trainIndex, ]
test_X <- X[-trainIndex,]

train_label <- label[trainIndex]
test_label <- label[-trainIndex]
```

```{r, echo=FALSE}

xgb_model <- xgboost(data = train_X, label = train_label, objective = "binary:logistic", nrounds=10)

```
As the logarithmic loss value decrease from 0.607468 to 0.394403, the model is improving and learning from the training data over time.

```{r, echo=FALSE}
predictions <- predict(xgb_model, newdata = test_X)
predicted_labels <- as.numeric(ifelse(predictions > 0.5, 1, 0))
accuracy <- mean(predicted_labels == test_label)
accuracy

```
According to the accuracy result, the model correctly predicted the label 71.06% of the time in the test data set.

```{r, echo=FALSE}
conf_matrix <- confusionMatrix(as.factor(predicted_labels), as.factor(test_label))
conf_matrix$table

```
**Confusion Matrix**：

True Negatives (TN): The model correctly identified 58 negatives (0s).

False Positives (FP): There were 55 instances where negatives (0s) were incorrectly identified as positives (1s).

False Negatives (FN): The model incorrectly predicted 239 positives (1s) as negatives (0s).

True Positives (TP): It correctly identified 664 positives (1s).

Using these, I calculate the metrics:

Accuracy: (TP+TN)/(TP+FP+FN+TN) = (664+58)/(664+55+239+58) = 722 / 1016 ≈ 0.711 or 71.1%.

Precision (Positive Predictive Value): TP/(TP+FP) = 664/(664+55) = 664 / 719 ≈ 0.923 or 92.3%.

Recall or Sensitivity (True Positive Rate): TP/(TP+FN) = 664/(664+239) = 664 / 903 ≈ 0.735 or 73.5%.

Specificity (True Negative Rate): TN/(TN+FP) = 58/(58+55) = 58 / 113 ≈ 0.513 or 51.3%.

```{r, echo=FALSE}
auroc <- roc(test_label, predictions)
auroc
```
The AUROC value is 0.6912, which means that a randomly chosen positive example (case) has 69.12% of chance to be ranked higher by the model than a randomly chosen negative example (control).

## Test model's performance on test trails from session 1 and session 18
```{r,echo=FALSE}
session <- list()
for(i in 1:1){
  session[[i]] <- readRDS("./Test Data/test1.rds")
  cat('Session', i, ':', session[[i]]$mouse_name, 'on', session[[i]]$date_exp, '\n')
}

get_trail_functional_data <- function(session_id, trail_id){
  spikes <- session[[session_id]]$spks[[trail_id]]
  if (any(is.na(spikes))){
    disp("value missing")
  }

  trail_bin_average <- matrix(colMeans(spikes), nrow = 1)
  colnames(trail_bin_average) <- binename
  trail_tibble  = as_tibble(trail_bin_average)%>% add_column("trail_id" = trail_id) %>% add_column("contrast_left"= session[[session_id]]$contrast_left[trail_id]) %>% add_column("contrast_right"= session[[session_id]]$contrast_right[trail_id]) %>% add_column("feedback_type"= session[[session_id]]$feedback_type[trail_id])
  
  trail_tibble
}
get_session_functional_data <- function(session_id){
  n_trail <- length(session[[session_id]]$spks)
  trail_list <- list()
  for (trail_id in 1:n_trail){
    trail_tibble <- get_trail_functional_data(session_id,trail_id)
    trail_list[[trail_id]] <- trail_tibble
  }
  session_tibble <- as_tibble(do.call(rbind, trail_list))
  session_tibble <- session_tibble %>% add_column("mouse_name" = session[[session_id]]$mouse_name) %>% add_column("date_exp" = session[[session_id]]$date_exp) %>% add_column("session_id" = session_id) 
  session_tibble
}

session_list = list()
for (session_id in 1: 1){
  session_list[[session_id]] <- get_session_functional_data(session_id)
}
full_functional_tibble1 <- as_tibble(do.call(rbind, session_list))
full_functional_tibble1$session_id <- as.factor(full_functional_tibble1$session_id )
full_functional_tibble1$contrast_diff <- abs(full_functional_tibble1$contrast_left-full_functional_tibble1$contrast_right)

full_functional_tibble1$success <- full_functional_tibble1$feedback_type == 1
full_functional_tibble1$success <- as.numeric(full_functional_tibble1$success)

predictive_feature <- c("contrast_right","contrast_left", "contrast_diff" ,binename)
head(full_functional_tibble1[predictive_feature])

predictive_1 <- full_functional_tibble1[predictive_feature]

label1 <- as.numeric(full_functional_tibble1$success)

test_1 <- model.matrix(~., predictive_1)



predictions <- predict(xgb_model, test_1)
predicted_labels <- as.numeric(ifelse(predictions > 0.5, 1, 0))
accuracy <- mean(predicted_labels == label1)
accuracy
conf_matrix <- confusionMatrix(as.factor(predicted_labels), as.factor(label1))
conf_matrix$table
auroc <- roc(label1, predictions)
auroc
```
### Output Interpretation

a. **Accuracy Analysis**: The model achieved an accuracy of 74% on the test dataset, which suggests it correctly predicted the label 74 times out of 100.

b. **Confusion Matrix** Insights:

True Negatives (TN): The model correctly predicted 6 negatives (0s).

False Positives (FP): There were 4 instances where the model incorrectly predicted negatives (0s) as positives (1s).

False Negatives (FN): The model incorrectly predicted 22 positives (1s) as negatives (0s).

True Positives (TP): The model correctly predicted 68 positives (1s).

Using these, I calculate the metrics:

Accuracy: (TP+TN)/(TP+FP+FN+TN) = (68+6)/(68+4+22+6) = 74 / 100 = 0.74 or 74%.

Precision: TP/(TP+FP) = 68/(68+4) = 68 / 72 ≈ 0.944 or 94.4%.

Recall or Sensitivity: TP/(TP+FN) = 68/(68+22) = 68 / 90 ≈ 0.756 or 75.6%.

Specificity: TN/(TN+FP) = 6/(6+4) = 6 / 10 = 0.6 or 60%.

c. **AUROC** : The AUROC value of 0.692 means that there's a 69.2% chance a randomly chosen positive instance is ranked higher by the model than a randomly chosen negative instance. 
```{r,echo=FALSE}
session <- list()
for(i in 1:1){
  session[[i]] <- readRDS("./Test Data/test2.rds")
  cat('Session', i, ':', session[[i]]$mouse_name, 'on', session[[i]]$date_exp, '\n')
}

get_trail_functional_data <- function(session_id, trail_id){
  spikes <- session[[session_id]]$spks[[trail_id]]
  if (any(is.na(spikes))){
    disp("value missing")
  }

  trail_bin_average <- matrix(colMeans(spikes), nrow = 1)
  colnames(trail_bin_average) <- binename
  trail_tibble  = as_tibble(trail_bin_average)%>% add_column("trail_id" = trail_id) %>% add_column("contrast_left"= session[[session_id]]$contrast_left[trail_id]) %>% add_column("contrast_right"= session[[session_id]]$contrast_right[trail_id]) %>% add_column("feedback_type"= session[[session_id]]$feedback_type[trail_id])
  
  trail_tibble
}
get_session_functional_data <- function(session_id){
  n_trail <- length(session[[session_id]]$spks)
  trail_list <- list()
  for (trail_id in 1:n_trail){
    trail_tibble <- get_trail_functional_data(session_id,trail_id)
    trail_list[[trail_id]] <- trail_tibble
  }
  session_tibble <- as_tibble(do.call(rbind, trail_list))
  session_tibble <- session_tibble %>% add_column("mouse_name" = session[[session_id]]$mouse_name) %>% add_column("date_exp" = session[[session_id]]$date_exp) %>% add_column("session_id" = session_id) 
  session_tibble
}

session_list = list()
for (session_id in 1: 1){
  session_list[[session_id]] <- get_session_functional_data(session_id)
}
full_functional_tibble2 <- as_tibble(do.call(rbind, session_list))
full_functional_tibble2$session_id <- as.factor(full_functional_tibble2$session_id )
full_functional_tibble2$contrast_diff <- abs(full_functional_tibble2$contrast_left-full_functional_tibble2$contrast_right)

full_functional_tibble2$success <- full_functional_tibble2$feedback_type == 1
full_functional_tibble2$success <- as.numeric(full_functional_tibble2$success)

predictive_feature <- c("contrast_right","contrast_left", "contrast_diff" ,binename)
head(full_functional_tibble2[predictive_feature])

predictive_2 <- full_functional_tibble2[predictive_feature]

label2 <- as.numeric(full_functional_tibble2$success)

test_2 <- model.matrix(~., predictive_2)


predictions <- predict(xgb_model, test_2)
predicted_labels <- as.numeric(ifelse(predictions > 0.5, 1, 0))
accuracy <- mean(predicted_labels == label2)
accuracy
conf_matrix <- confusionMatrix(as.factor(predicted_labels), as.factor(label2))
conf_matrix$table
auroc <- roc(label2, predictions)
auroc

```

### Output Interpretation

a. **Overall Accuracy**: The model achieved an accuracy of 72%, which indicates it correctly predicted the outcome for 72 out of 100 instances in the test dataset.

b. **Confusion Matrix** Analysis:

True Negatives (TN): The model correctly identified 3 instances where the outcome was negative (0).

False Positives (FP): There were 4 instances where the model incorrectly predicted a negative outcome as positive (1).

False Negatives (FN): The model incorrectly predicted 24 instances of positive outcomes as negative.

True Positives (TP): The model successfully identified 69 instances of positive outcomes correctly.

Using these values, I can calculate the following metrics:

Accuracy: (TP + TN) / (TP + FP + FN + TN) = (69 + 3) / (69 + 4 + 24 + 3) = 72 / 100 = 0.72 or 72%.

Precision (Positive Predictive Value): TP / (TP + FP) = 69 / (69 + 4) = 69 / 73 ≈ 0.945 or 94.5%. This indicates the model is highly precise when it predicts a positive outcome.

Recall (Sensitivity or True Positive Rate): TP / (TP + FN) = 69 / (69 + 24) = 69 / 93 ≈ 0.742 or 74.2%. This suggests the model correctly identifies 74.2% of all actual positives but misses about 25.8%.

Specificity (True Negative Rate): TN / (TN + FP) = 3 / (3 + 4) = 3 / 7 ≈ 0.429 or 42.9%. The model's ability to correctly identify actual negatives is less reliable, with a specificity of only 42.9%.

c. **AUROC**: The AUROC value is 0.6144, meaning the model has a 61.44% chance of ranking a randomly chosen positive instance higher than a randomly chosen negative one. 

# Discussion

In previous analysis, I observed that while the model had a good start, its performance highlighted several deficiencies: For example, for the model's accuracy is at 71.06%, which indicates a potential over fitting issue; a low recall rate suggested a challenge in correctly identifying all relevant cases. The AUROC value of 0.6912, which implies there is considerable room for improvement in the model's ability to distinguish between classes. 

In addition, the test on two more dataset shows more about my model. The model's performance on Session 1 shows better accuracy (74%) and a higher AUROC (0.692) compared to Session 18, where accuracy drops slightly to 72% and the AUROC to 0.6144. This indicates the model is generally more effective in predicting outcomes in Session 1. In both sessions, the model demonstrates high precision, indicating when it predicts positives, it's often correct. However, the model struggles with recall, especially in Session 18, which means that it misses a significant number of true positives. The low specificity in Session 18 also suggesst challenges in correctly identifying true negatives.

To improve the model, I believe we should enhance recall and specificity. In future practice, we can use techniques such as feature engineering to include more predictors. W can also adjust the model's threshold to balance sensitivity and specificity. Additionally, we can add session-specific characteristics in the model, which could address the performance variability between sessions. 

# Reference {-}


Steinmetz, N.A., Zatka-Haas, P., Carandini, M. et al. Distributed coding of choice, action and engagement across the mouse brain. Nature 576, 266–273 (2019). https://doi.org/10.1038/s41586-019-1787-x


